{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znku3TNhZY2m"
   },
   "source": [
    "# Final Evalaution of range data model\n",
    "\n",
    "The model was chosen in 11, the data input in notebook 12, the causal vs non-causal design compared in notebook 13, regularization investigated in notebook 14 and the hyperparameters chosen from 15.\n",
    "\n",
    "First the model is trained on subjects A, B, D, E, F. 80% of this data is used for training whilst the remaining 20% is used for validation which allows the use of erly stopping to prevent overfitting.\n",
    "\n",
    "The model is then evaluated on the test subject C. Up until this point the model has not been exposed to this data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZxhO7V0ZHUE"
   },
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGNeUj-JDXhs"
   },
   "outputs": [],
   "source": [
    "# Needed to allow editing using PyCharm etc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwLbqieVYIJt"
   },
   "source": [
    "The following cell is needed for compatibility when using both CoLab and Local Jupyter notebook. It sets the appropriate file path for the data and also installs local packages such as models and data_loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3XeU0HtoDXh6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "if path == '/content':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    BASE_PATH = '/content/gdrive/My Drive/Level-4-Project/'\n",
    "#     !cd gdrive/My\\ Drive/Level-4-Project/ && pip install --editable .\n",
    "    os.chdir('gdrive/My Drive/Level-4-Project/')\n",
    "    \n",
    "elif path == 'D:\\\\Google Drive\\\\Level-4-Project\\\\notebooks':\n",
    "    BASE_PATH = \"D:/Google Drive/Level-4-Project/\"\n",
    "    \n",
    "elif path == \"/export/home/2192793m\":\n",
    "    BASE_PATH = \"/export/home/2192793m/Level-4-Project/\"\n",
    "    \n",
    "    \n",
    "DATA_PATH_MTI = BASE_PATH + 'data/processed/range_FFT/3/MTI_applied/' # not used\n",
    "DATA_PATH_NO_MTI = BASE_PATH + 'data/processed/range_FFT/3/MTI_not_applied/'\n",
    "\n",
    "RESULTS_PATH = BASE_PATH + 'results/range_data_model_final_evaluation/'\n",
    "HYPERPARAMETER_PATH = BASE_PATH + 'results/range_data_model_hyperparameter_search/'\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "    \n",
    "MODEL_PATH = BASE_PATH + 'models/range_data_model_final_evaluation/'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QW7Fa5jTCDXo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.callbacks import History, ModelCheckpoint, CSVLogger\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras.layers import Input, Conv1D, Multiply, Add, Reshape, Activation, AveragePooling1D, Lambda, Flatten, Dense,GlobalAveragePooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import History, ModelCheckpoint, EarlyStopping\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tk_HAyYKYrI8"
   },
   "outputs": [],
   "source": [
    "# needed for CheckpointSaver\n",
    "# https://github.com/scikit-optimize/scikit-optimize/issues/678\n",
    "# ! pip install git+https://github.com/scikit-optimize/scikit-optimize/ \n",
    "    \n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "from skopt import dump, load\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxIKU3-fTUy7"
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sj59Pvxv5CX9"
   },
   "outputs": [],
   "source": [
    "# Load in data dictionary.\n",
    "# This does not load in any actual data,\n",
    "# just the dictionary with the names of the files and their associated labels\n",
    "with open(DATA_PATH_NO_MTI + \"index.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQIwQ8EACdIQ"
   },
   "outputs": [],
   "source": [
    "def convert_label_to_int(label):\n",
    "    if label == \"walking\":\n",
    "        return 0\n",
    "    if label == \"pushing\":\n",
    "        return 1\n",
    "    if label == \"sitting\":\n",
    "        return 2\n",
    "    if label == \"pulling\":\n",
    "        return 3\n",
    "    if label == \"circling\":\n",
    "        return 4\n",
    "    if label == \"clapping\":\n",
    "        return 5\n",
    "    if label == \"bending\":\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4fd5mwu11f9"
   },
   "outputs": [],
   "source": [
    "target_names = [\"walking\", \"pushing\", \"sitting\", \"pulling\", \"circling\", \"clapping\", \"bending\"]\n",
    "nb_classes = len(target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ah1RGSSTYfQ"
   },
   "source": [
    "## DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gOcp1JeSop2"
   },
   "outputs": [],
   "source": [
    "'''Based on code from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly'''\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\"\"\"\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(3000),\n",
    "                 n_classes=7, shuffle=False, data_directory='data/',\n",
    "                 bin_range=(0,60), take_average=False, every_second_cell=False):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.data_directory = data_directory\n",
    "        self.bin_range=bin_range\n",
    "        self.take_average = take_average\n",
    "        self.every_second_cell = every_second_cell\n",
    "        self.indexes = None\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            if self.take_average:\n",
    "                X[i,] = abs(np.average(np.load(self.data_directory + ID), axis=1)[:,np.newaxis])\n",
    "                \n",
    "            elif self.every_second_cell:\n",
    "                X[i,] = abs(np.load(self.data_directory + ID))[:,::2]\n",
    "                \n",
    "            else:\n",
    "                X[i,] = abs(np.load(self.data_directory + ID))[:,self.bin_range[0]:self.bin_range[1]]\n",
    "                \n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88LpWZJeTfj2"
   },
   "source": [
    "## Model: Wavenet model adapted based on interpretation from Wavenet Paper\n",
    "\n",
    "Keras implementation of wavenet model taken from https://github.com/basveeling/wavenet and https://github.com/mjpyeon/wavenet-classifier\n",
    "\n",
    "This model has then been adapted to the classification task based on the intrustions from the paper \"WAVENET: A GENERATIVE MODEL FOR RAW AUDIO\" (https://arxiv.org/pdf/1609.03499.pdf)\n",
    "\n",
    "Specifically:\n",
    "\"For this task we added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160× downsampling).  The pooling layer was followed by a few non-causal convolutions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetClassifier:\n",
    "    def __init__(self, input_shape, output_shape, kernel_size=2, dilation_depth=9, nb_stacks=1, nb_filters=40,\n",
    "                 pool_size=80, kernel_size_2=100, use_skip_connections=True, causal=True, residual_l2=0.001,\n",
    "                 conv_l2=0.001, fully_l2=0.001, use_batch_norm=True, num_dense_nodes=512):\n",
    "\n",
    "        self.activation = 'softmax'\n",
    "        self.pool_size = pool_size\n",
    "        self.kernel_size_2 = kernel_size_2 # kernel size for later conV 1d (not dilated)\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size # kernel size for dilated  layers\n",
    "        self.dilation_depth = dilation_depth\n",
    "        self.nb_filters = nb_filters\n",
    "        self.residual_l2 = residual_l2 # l2 value for residual layers\n",
    "        self.conv_l2 = conv_l2 # l2 value for stack of standard conv layers\n",
    "        self.fully_l2 = fully_l2 # l2 value for fully connected layer\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.num_dense_nodes = num_dense_nodes\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        if causal:\n",
    "            self.padding = 'causal'\n",
    "        else:\n",
    "            self.padding = 'same'\n",
    "\n",
    "        if len(input_shape) == 1:\n",
    "            self.expand_dims = True\n",
    "        elif len(input_shape) == 2:\n",
    "            self.expand_dims = False\n",
    "        else:\n",
    "            print('ERROR: wrong input shape')\n",
    "            sys.exit()\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def residual_block(self, x, i, stack_nb):\n",
    "        original_x = x\n",
    "        tanh_out = Conv1D(self.nb_filters, self.kernel_size, dilation_rate=2 ** i, padding=self.padding,\n",
    "                          name='dilated_conv_%d_tanh_s%d' % (2 ** i, stack_nb), activation='tanh',\n",
    "                          kernel_regularizer=l2(self.residual_l2))(x)\n",
    "        sigm_out = Conv1D(self.nb_filters, self.kernel_size, dilation_rate=2 ** i, padding=self.padding,\n",
    "                          name='dilated_conv_%d_sigm_s%d' % (2 ** i, stack_nb), activation='sigmoid',\n",
    "                          kernel_regularizer=l2(self.residual_l2))(x)\n",
    "        x = Multiply(name='gated_activation_%d_s%d' % (i, stack_nb))([tanh_out, sigm_out])\n",
    "\n",
    "        res_x = Conv1D(self.nb_filters, 1, padding='same', kernel_regularizer=l2(self.residual_l2))(x)\n",
    "        skip_x = Conv1D(self.nb_filters, 1, padding='same', kernel_regularizer=l2(self.residual_l2))(x)\n",
    "        res_x = Add()([original_x, res_x])\n",
    "        return res_x, skip_x\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = Input(shape=self.input_shape, name='input_part')\n",
    "        out = input_layer\n",
    "        skip_connections = []\n",
    "        out = Conv1D(self.nb_filters, self.kernel_size,\n",
    "                     dilation_rate=1,\n",
    "                     padding=self.padding,\n",
    "                     name='initial_causal_conv'\n",
    "                     )(out)\n",
    "        for stack_nb in range(self.nb_stacks):\n",
    "            for i in range(0, self.dilation_depth + 1):\n",
    "                out, skip_out = self.residual_block(out, i, stack_nb)\n",
    "                skip_connections.append(skip_out)\n",
    "\n",
    "        if self.use_skip_connections:\n",
    "            out = Add()(skip_connections)\n",
    "        out = Activation('relu')(out)\n",
    "        # added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames\n",
    "        # spanning 10 milliseconds (160× downsampling)\n",
    "        # mean pooling layer adjust pool_size_1 to change downsampling\n",
    "                \n",
    "        out = AveragePooling1D(self.pool_size, padding='same', name='mean_pooling_layer_downsampling')(out)\n",
    "\n",
    "        # few non-causal convolutions\n",
    "        # In notebooks 11, 12 and 13 self.kernel_size_2 was incorrectly represented as pooling sizes.\n",
    "        out = Conv1D(self.nb_filters, self.kernel_size_2, strides=2, padding='same', activation='relu',\n",
    "                     kernel_regularizer=l2(self.conv_l2))(out)\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "            out = BatchNormalization()(out)\n",
    "        \n",
    "        out = Conv1D(self.nb_filters, self.kernel_size_2, strides=2, padding='same', activation='relu',\n",
    "                     kernel_regularizer=l2(self.conv_l2))(out)\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "            out = BatchNormalization()(out)\n",
    "        \n",
    "        out = Conv1D(self.output_shape, self.kernel_size_2, strides=2, padding='same', activation='relu',\n",
    "                     kernel_regularizer=l2(self.conv_l2))(out)\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "            out = BatchNormalization()(out)\n",
    "        \n",
    "        out = Conv1D(self.output_shape, self.kernel_size_2, strides=2, padding='same', activation='relu',\n",
    "                     kernel_regularizer=l2(self.conv_l2))(out)\n",
    "\n",
    "        if self.use_batch_norm:\n",
    "            out = BatchNormalization()(out)\n",
    "            \n",
    "        out = Flatten()(out)\n",
    "        out = Dense(num_dense_nodes, activation='relu', kernel_regularizer=l2(self.fully_l2))(out)\n",
    "        out = Dense(self.output_shape, activation='softmax')(out)\n",
    "\n",
    "        return Model(input_layer, out)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def get_receptive_field(self):\n",
    "        return self.nb_stacks * (self.kernel_size + (2*(self.kernel_size - 1) * ((2**self.dilation_depth) - 1))) - (self.nb_stacks - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_RESULTS_TEST_C = True\n",
    "SAVE_GRAPHS_TEST_C = True\n",
    "\n",
    "SAVE_RESULTS_20 = True\n",
    "SAVE_GRAPHS_20 = True\n",
    "\n",
    "SAVE_RESULTS_6_FOLD = True\n",
    "SAVE_GRAPHS_6_FOLD = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for loading results (objective needs to be defined)\n",
    "def objective():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = ['n_filters', 'kernel_size', 'dilation_depth', 'number_of_stacks',\n",
    "              'pool_size', 'kernel_size_2', 'early_stopping_patience']\n",
    "\n",
    "# res_gp = load(HYPERPARAMETER_PATH + \"res_gp_complete.pkl\")\n",
    "res_gp = load(HYPERPARAMETER_PATH + \"res_gp_checkpoint.pkl\")\n",
    "\n",
    "parameters = res_gp.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_filters: 32\n",
      "kernel_size: 3\n",
      "dilation_depth: 9\n",
      "number_of_stacks: 3\n",
      "pool_size: 10\n",
      "kernel_size_2: 7\n",
      "early_stopping_patience: 4\n"
     ]
    }
   ],
   "source": [
    "for index, parameter in enumerate(parameters):\n",
    "    print(dimensions[index] + \":\", parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = int(parameters[0])\n",
    "kernel_size = int(parameters[1])\n",
    "dilation_depth = int(parameters[2])\n",
    "number_of_stacks = int(parameters[3])\n",
    "pool_size = int(parameters[4])\n",
    "kernel_size_2 = int(parameters[5])\n",
    "early_stopping_patience = int(parameters[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxXVIaedsPxK"
   },
   "outputs": [],
   "source": [
    "bin_range = (0,63)\n",
    "data_shape = (3000, 32)\n",
    "\n",
    "activation = 'softmax'\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "num_dense_nodes = 512\n",
    "\n",
    "residual_l2 = 0.001\n",
    "conv_l2 = 0.001\n",
    "fully_l2 = 0.001\n",
    "use_batch_norm = False\n",
    "\n",
    "# Parameters for data generators\n",
    "data_gen_params = {'dim': data_shape,\n",
    "                   'batch_size': batch_size,\n",
    "                   'n_classes': nb_classes,\n",
    "                   'data_directory': DATA_PATH_NO_MTI,\n",
    "                   'bin_range': bin_range,\n",
    "                   'every_second_cell': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnc = WaveNetClassifier((data_shape), (nb_classes), kernel_size=kernel_size,\n",
    "                        dilation_depth=dilation_depth, nb_stacks=number_of_stacks,\n",
    "                        nb_filters=n_filters,\n",
    "                        pool_size=pool_size, kernel_size_2=kernel_size_2,\n",
    "                        residual_l2=residual_l2, conv_l2=conv_l2, fully_l2=fully_l2,\n",
    "                        use_batch_norm=use_batch_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhFcRSbwZCCw"
   },
   "source": [
    "# Evaluating on subject C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test_by_subject(data, test_subject):\n",
    "    labels = {}\n",
    "    partition = {'train':[], 'validation':[], 'test':[]} # contains list of training and validation ID's\n",
    "\n",
    "    for subject_letter, actions in data.items():\n",
    "        for action, results in actions.items():\n",
    "            for result in results:\n",
    "                # do not split the test subject\n",
    "                if subject_letter == test_subject:\n",
    "                    for row in result:\n",
    "                        partition[\"test\"].append(row)\n",
    "                        labels[row] = convert_label_to_int(action)\n",
    "                        \n",
    "                # if not test subject split data 80:20 (in consectutive chunks)\n",
    "                else:            \n",
    "                    res = np.array(result)\n",
    "                    # Split into 5 folds then take 1 fold for 20%\n",
    "                    split_actions = np.array_split(res, 5)\n",
    "                    for fold in range(5):\n",
    "                        data = split_actions[fold]\n",
    "                        if fold == 0:\n",
    "                            for row in data:\n",
    "                                partition[\"validation\"].append(row)\n",
    "                                labels[row] = convert_label_to_int(action)\n",
    "\n",
    "                        else:\n",
    "                            for row in data:\n",
    "                                partition[\"train\"].append(row)\n",
    "                                labels[row] = convert_label_to_int(action)       \n",
    "                                \n",
    "    return {\"complete_index\": labels, \"partition\":partition}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = split_train_val_test_by_subject(data, \"C\")\n",
    "complete_index = data_split[\"complete_index\"]\n",
    "partition = data_split[\"partition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnc.build_model()\n",
    "model = wnc.get_model()\n",
    "\n",
    "training_generator = DataGenerator(partition['train'], complete_index, **params, shuffle=True)\n",
    "validation_generator = DataGenerator(partition['validation'], complete_index, **params, shuffle=False)\n",
    "\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callback_list = []\n",
    "# -1 used to represent no early stopping\n",
    "if early_stopping_patience != -1:\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience)\n",
    "    callback_list.append(early_stopping)\n",
    "\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                              validation_data=validation_generator,\n",
    "                              epochs=epochs,\n",
    "                              callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(partition['test'], complete_index, **params, shuffle=False)\n",
    "\n",
    "predictions = model.predict_generator(test_generator)\n",
    "predictions = np.argmax(predictions,axis=1)\n",
    "true_labels = []\n",
    "for ID in test_generator.list_IDs:\n",
    "    true_labels.append(complete_index[ID])\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "true_labels = true_labels[:len(predictions)]\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "evaluation = model.evaluate_generator(test_generator)\n",
    "\n",
    "results = {\n",
    "    \"confusion_matrix\": cm,\n",
    "    \"loss\": evaluation[0],\n",
    "    \"accuracy\": evaluation[1],\n",
    "    \"history\": history.history\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_RESULTS_TEST_C:\n",
    "    with open(RESULTS_PATH + \"results_test_c.pkl\", 'wb') as results_file:\n",
    "        pickle.dump(results, results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH + \"results_test_c.pkl\", 'rb') as results_file:\n",
    "    results = pickle.load(results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on subset of the data (all subjects) 5-Fold\n",
    "use 60% for training, 20% for validation (early stopping) and the final 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_fold(data, test_fold, validation_fold, total_folds=5):\n",
    "    labels = {}\n",
    "    partition = {'train':[], 'validation':[], 'test':[]}\n",
    "\n",
    "    for user_letter, actions in data.items():\n",
    "        for action, results in actions.items():\n",
    "            for result in results:\n",
    "                res = np.array(result)\n",
    "                # Split into 5 folds then take 1 fold for 20%\n",
    "                split_actions = np.array_split(res, 5)\n",
    "                for fold in range(5):\n",
    "                    data = split_actions[fold]\n",
    "                    if fold == test_fold:\n",
    "                        for row in data:\n",
    "                            partition[\"test\"].append(row)\n",
    "                            labels[row] = convert_label_to_int(action)\n",
    "                            \n",
    "                    elif fold == validation_fold:\n",
    "                        for row in data:\n",
    "                            partition[\"validation\"].append(row)\n",
    "                            labels[row] = convert_label_to_int(action)\n",
    "\n",
    "                    else:\n",
    "                        for row in data:\n",
    "                            partition[\"train\"].append(row)\n",
    "                            labels[row] = convert_label_to_int(action) \n",
    "                            \n",
    "    return {\"complete_index\": labels, \"partition\":partition}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_results = {}\n",
    "number_of_folds = 5\n",
    "for fold in range(number_of_folds):\n",
    "    k_fold_results[str(fold)] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy = 0\n",
    "average_loss = 0\n",
    "for fold in range(number_of_folds):\n",
    "    test_fold = fold\n",
    "    validation_fold = (test_fold + 1) % number_of_folds\n",
    "    print(\"Fold:\", fold)\n",
    "    \n",
    "    data_split = split_data_by_fold(data, test_fold, validation_fold, number_of_folds)\n",
    "    complete_index = data_split[\"complete_index\"]\n",
    "    partition = data_split[\"partition\"]\n",
    "    \n",
    "    wnc.build_model()\n",
    "    model = wnc.get_model()\n",
    "\n",
    "    training_generator = DataGenerator(partition['train'], complete_index, **params, shuffle=True)\n",
    "    validation_generator = DataGenerator(partition['validation'], complete_index, **params, shuffle=False)\n",
    "    test_generator = DataGenerator(partition['test'], complete_index, **params, shuffle=False)\n",
    "\n",
    "    model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    callback_list = []\n",
    "    # -1 used to represent no early stopping\n",
    "    if early_stopping_patience != -1:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience)\n",
    "        callback_list.append(early_stopping)\n",
    "\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callback_list)\n",
    "\n",
    "    evaluation = model.evaluate_generator(test_generator)\n",
    "    loss = evaluation[0]\n",
    "    accuracy = evaluation[1]\n",
    "\n",
    "    average_accuracy += accuracy\n",
    "    average_loss += loss\n",
    "    k_fold_results[str(fold)][\"accuracy\"] = accuracy\n",
    "    k_fold_results[str(fold)][\"loss\"] = loss\n",
    "    \n",
    "k_fold_results[\"average_accuracy\"] = average_accuracy/number_of_folds\n",
    "k_fold_results[\"average_loss\"] = average_loss/number_of_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_RESULTS_20:\n",
    "    with open(RESULTS_PATH + \"results_test_20.pkl\", 'wb') as results_file:\n",
    "        pickle.dump(k_fold_results, results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH + \"results_test_20.pkl\", 'rb') as results_file:\n",
    "    k_fold_results = pickle.load(results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Accuracy:\", k_fold_results[\"average_accuracy\"]*100)\n",
    "print(\"Average Loss:\", k_fold_results[\"average_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Fold Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for fold in range(number_of_folds):\n",
    "    accuracies.append(k_fold_results[str(fold)][\"accuracy\"]*100)\n",
    "\n",
    "plt.boxplot(accuracies, zorder=3)\n",
    "plt.title(\"Fold Performance Distribution Comparison\")\n",
    "# plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Classification Accuracy (%)\")\n",
    "plt.grid(axis='y', zorder=0)\n",
    "if SAVE_GRAPHS_20:\n",
    "     plt.savefig(RESULTS_PATH + \"boxplot_20.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six-Fold Cross-Validation\n",
    "One fold for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_six_fold = {}\n",
    "for subject in subjects:\n",
    "    results_six_fold[subject] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy = 0\n",
    "average_loss = 0\n",
    "for subject in subjects:\n",
    "    data_split = split_train_val_test_by_subject(data, subject)\n",
    "    complete_index = data_split[\"complete_index\"]\n",
    "    partition = data_split[\"partition\"]\n",
    "    \n",
    "    wnc.build_model()\n",
    "    model = wnc.get_model()\n",
    "\n",
    "    training_generator = DataGenerator(partition['train'], complete_index, **params, shuffle=True)\n",
    "    validation_generator = DataGenerator(partition['validation'], complete_index, **params, shuffle=False)\n",
    "    test_generator = DataGenerator(partition['test'], complete_index, **params, shuffle=False)\n",
    "\n",
    "    model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    callback_list = []\n",
    "    # -1 used to represent no early stopping\n",
    "    if early_stopping_patience != -1:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience)\n",
    "        callback_list.append(early_stopping)\n",
    "\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callback_list)\n",
    "\n",
    "    evaluation = model.evaluate_generator(test_generator)\n",
    "    loss = evaluation[0]\n",
    "    accuracy = evaluation[1]\n",
    "        \n",
    "    average_accuracy += accuracy\n",
    "    average_loss += loss\n",
    "    results_six_fold[subject][\"accuracy\"] = accuracy\n",
    "    results_six_fold[subject][\"loss\"] = loss\n",
    "\n",
    "results_six_fold[\"avg_loss\"] = average_loss/len(subjects)\n",
    "results_six_fold[\"avg_acc\"] = average_accuracy/len(subjects)\n",
    "print(\"Average Loss:\", average_loss/len(subjects))\n",
    "print(\"Average Accuracy:\", average_accuracy/len(subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_RESULTS_6_FOLD:\n",
    "    with open(RESULTS_PATH + \"results_6_fold.pkl\", 'wb') as results_file:\n",
    "        pickle.dump(results_six_fold, results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH + \"results_6_fold.pkl\", 'rb') as results_file:\n",
    "    results_six_fold = pickle.load(results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Accuracy:\", results_six_fold[\"avg_acc\"]*100)\n",
    "print(\"Average Loss:\", results_six_fold[\"avg_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for subject in subjects:\n",
    "    accuracies.append(results_six_fold[subject][\"accuracy\"]*100)\n",
    "\n",
    "plt.boxplot(accuracies, zorder=3)\n",
    "plt.title(\"Fold Performance Distribution Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Classification Accuracy (%)\")\n",
    "plt.grid(axis='y', zorder=0)\n",
    "if SAVE_GRAPHS_6_FOLD:\n",
    "     plt.savefig(RESULTS_PATH + \"boxplot_6_fold.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(subjects)), accuracies, zorder=3)\n",
    "plt.xticks(range(len(subjects)), subjects)\n",
    "plt.xlabel(\"Subject (fold)\")\n",
    "plt.ylabel(\"Classification Accuracy (%)\")\n",
    "plt.title(\" Fold Accuracy Distribution\")\n",
    "plt.grid(axis='y', zorder=0)\n",
    "if SAVE_GRAPHS_6_FOLD:\n",
    "    plt.savefig(RESULTS_PATH + \"fold_accuracy_distribution_6_fold.pdf\", format='pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "12_range_data_model_hyperparameter_search.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
