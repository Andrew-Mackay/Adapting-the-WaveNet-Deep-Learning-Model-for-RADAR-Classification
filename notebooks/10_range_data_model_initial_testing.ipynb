{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_range_data_model_initial_testing.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"znku3TNhZY2m","colab_type":"text"},"cell_type":"markdown","source":["# Exploring how models designed for the range dataset perform.\n","\n","The idea of this notebook is just to develop models and get a rough idea of how well they perform.\n","\n","It is also used to get an idea for performance, training time/epoch, with and without MTI filter, number of bins to use and number of layers.\n"]},{"metadata":{"colab_type":"text","id":"bZxhO7V0ZHUE"},"cell_type":"markdown","source":["## Notebook setup"]},{"metadata":{"colab_type":"code","id":"TGNeUj-JDXhs","colab":{}},"cell_type":"code","source":["# Needed to allow editing using PyCharm etc\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"QwLbqieVYIJt"},"cell_type":"markdown","source":["The following cell is needed for compatibility when using both CoLab and Local Jupyter notebook. It sets the appropriate file path for the data and also installs local packages such as models and data_loading."]},{"metadata":{"colab_type":"code","id":"3XeU0HtoDXh6","outputId":"e2eec04b-2c0e-467f-a1d7-cf3f11322c1b","executionInfo":{"status":"ok","timestamp":1547753943275,"user_tz":0,"elapsed":18232,"user":{"displayName":"Andrew Mackay","photoUrl":"https://lh3.googleusercontent.com/-24hiGmdxZDE/AAAAAAAAAAI/AAAAAAAAL_I/RW7nqM11LkM/s64/photo.jpg","userId":"06804410358976473893"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["import os\n","path = os.getcwd()\n","if path == '/content':\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    BASE_PATH = '/content/gdrive/My Drive/Level-4-Project/'\n","    !cd gdrive/My\\ Drive/Level-4-Project/ && pip install --editable .\n","    os.chdir('gdrive/My Drive/Level-4-Project/')\n","    \n","elif path == 'D:\\\\Google Drive\\\\Level-4-Project\\\\notebooks\\\\wavenet':\n","    BASE_PATH = \"D:/Google Drive/Level-4-Project/\"\n","    \n","elif path == \"/export/home/2192793m\":\n","    BASE_PATH = \"/export/home/2192793m/Level-4-Project/\"\n","    \n","    \n","DATA_PATH_MTI = BASE_PATH + 'data/processed/range_FFT/3/MTI_applied/\n","DATA_PATH_NO_MTI = BASE_PATH + 'data/processed/range_FFT/3/MTI_not_applied/\n","\n","RESULTS_PATH = BASE_PATH + 'results/range_data_model_initial_testing/'\n","if not os.path.exists(RESULTS_PATH):\n","    os.makedirs(RESULTS_PATH)\n","    \n","\n","\n","MODEL_PATH = BASE_PATH + 'models/wavenet/range_fft/bins_5_25_mti/test_4/'\n","    \n","from src.visualization import visualize"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Obtaining file:///content/gdrive/My%20Drive/Level-4-Project\n","Installing collected packages: src\n","  Found existing installation: src 0.1.0\n","    Can't uninstall 'src'. No files were found to uninstall.\n","  Running setup.py develop for src\n","Successfully installed src\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"colab_type":"code","id":"QW7Fa5jTCDXo","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","from sklearn.metrics import classification_report, confusion_matrix\n","from keras import metrics\n","from keras import optimizers\n","from keras.callbacks import History, ModelCheckpoint, CSVLogger\n","from keras.models import load_model\n","from keras.utils import Sequence, to_categorical\n","from keras.layers import Input, Conv1D, Multiply, Add, Reshape, Activation, AveragePooling1D, Lambda, Flatten, Dense\n","from keras.models import load_model, Model\n","from keras.callbacks import History, ModelCheckpoint\n","\n","import pandas as pd\n","import sys\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vxIKU3-fTUy7","colab_type":"text"},"cell_type":"markdown","source":["## Data Setup"]},{"metadata":{"id":"sj59Pvxv5CX9","colab_type":"code","colab":{}},"cell_type":"code","source":["# Load in data dictionary.\n","# This does not load in any actual data,\n","# just the dictionary with the names of the files and their associated labels\n","with open(DATA_PATH + \"index.pkl\", \"rb\") as file:\n","    data = pickle.load(file)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WYNFp-60ZFe2","colab_type":"code","colab":{}},"cell_type":"code","source":["Remove user C as this user is reserved for the test set\n","try:\n","    del data[\"C\"]\n","except KeyError:\n","    print (\"Key 'C' not found\")"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"TQIwQ8EACdIQ","colab":{}},"cell_type":"code","source":["def convert_label_to_int(label):\n","    if label == \"walking\":\n","        return 0\n","    if label == \"pushing\":\n","        return 1\n","    if label == \"sitting\":\n","        return 2\n","    if label == \"pulling\":\n","        return 3\n","    if label == \"circling\":\n","        return 4\n","    if label == \"clapping\":\n","        return 5\n","    if label == \"bending\":\n","        return 6"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zN4lXwZg7vv_","colab_type":"code","colab":{}},"cell_type":"code","source":["labels = {}\n","partition = {'train':[], 'validation':[]} # contains list of training and validation ID's\n","validation_user = \"B\" # use user B for validation\n","\n","for user_letter, actions in data.items():\n","    for action, results in actions.items():\n","        for result in results:\n","            for row in result:\n","                if user_letter == validation_user:\n","                    partition[\"validation\"].append(row)\n","                    labels[row] = convert_label_to_int(action)\n","\n","                else:\n","                    partition[\"train\"].append(row)\n","                    labels[row] = convert_label_to_int(action)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n4fd5mwu11f9","colab_type":"code","colab":{}},"cell_type":"code","source":["target_names = [\"walking\", \"pushing\", \"sitting\", \"pulling\", \"circling\", \"clapping\", \"bending\"]\n","nb_classes = len(target_names)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0ah1RGSSTYfQ","colab_type":"text"},"cell_type":"markdown","source":["## DataGenerator"]},{"metadata":{"id":"_gOcp1JeSop2","colab_type":"code","colab":{}},"cell_type":"code","source":["'''Based on code from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly'''\n","\n","class DataGenerator(Sequence):\n","    \"\"\"Generates data for Keras\"\"\"\n","    def __init__(self, list_IDs, labels, batch_size=32, dim=(3000),\n","                 n_classes=7, shuffle=False, data_directory='data/',\n","                 bin_range=(0,60)):\n","        \"\"\"Initialization\"\"\"\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.labels = labels\n","        self.list_IDs = list_IDs\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.data_directory = data_directory\n","        self.bin_range=bin_range\n","        self.indexes = None\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        \"\"\"Denotes the number of batches per epoch\"\"\"\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        \"\"\"Generate one batch of data\"\"\"\n","\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        \"\"\"Updates indexes after each epoch\"\"\"\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        \"\"\"Generates data containing batch_size samples\"\"\"\n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim))\n","\n","        y = np.empty((self.batch_size), dtype=int)\n","\n","        # Generate data\n","        for i, ID in enumerate(list_IDs_temp):\n","            # Store sample\n","            X[i,] = abs(np.load(self.data_directory + ID))[:,self.bin_range[0]:self.bin_range[1]]\n","            # Store class\n","            y[i] = self.labels[ID]\n","\n","        return X, to_categorical(y, num_classes=self.n_classes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"88LpWZJeTfj2","colab_type":"text"},"cell_type":"markdown","source":["## Model 1: Wavenet model adapted based on interpretation from Wavenet Paper\n","\n","Keras implementation of wavenet model taken from https://github.com/basveeling/wavenet\n","\n","This model has then been adapted to the classification task based on the intrustions from the paper \"WAVENET: A GENERATIVE MODEL FOR RAW AUDIO\" (https://arxiv.org/pdf/1609.03499.pdf)\n","\n","Specifically:\n","\"For this task we added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160× downsampling).  The pooling layer was followed by a few non-causal convolutions.\""]},{"metadata":{"id":"StWO3HS4bY5A","colab_type":"text"},"cell_type":"markdown","source":["### Model"]},{"metadata":{"id":"RMveaN0ITfAP","colab_type":"code","colab":{}},"cell_type":"code","source":["class WaveNetClassifier:\n","    def __init__(self, input_shape, output_shape, kernel_size=2, dilation_depth=9, nb_stacks=1, nb_filters=40,\n","                 pool_size_1=80, pool_size_2=100, use_bias=False, use_skip_connections=False):\n","        \"\"\"\n","        Parameters:\n","          input_shape: (tuple) tuple of input shape. (e.g. If input is 6s raw waveform with sampling rate = 16kHz, (96000,) is the input_shape)\n","          output_shape: (tuple)tuple of output shape. (e.g. If we want classify the signal into 100 classes, (100,) is the output_shape)\n","          kernel_size: (integer) kernel size of convolution operations in residual blocks\n","          dilation_depth: (integer) type total depth of residual blocks\n","          n_filters: (integer) # of filters of convolution operations in residual blocks\n","          load: (bool) load previous WaveNetClassifier or not\n","          load_dir: (string) the directory where the previous model exists\n","        \"\"\"\n","        self.activation = 'softmax'\n","        self.scale_ratio = 1\n","        self.pool_size_1 = pool_size_1\n","        self.pool_size_2 = pool_size_2\n","        self.nb_stacks = nb_stacks\n","        self.kernel_size = kernel_size\n","        self.dilation_depth = dilation_depth\n","        self.nb_filters = nb_filters\n","        self.use_bias = use_bias\n","        self.use_skip_connections = use_skip_connections\n","        self.input_shape = input_shape\n","        self.output_shape = output_shape\n","\n","        if len(input_shape) == 1:\n","            self.expand_dims = True\n","        elif len(input_shape) == 2:\n","            self.expand_dims = False\n","        else:\n","            print('ERROR: wrong input shape')\n","            sys.exit()\n","\n","        self.model = self.build_model()\n","\n","    def residual_block(self, x, i, stack_nb):\n","        original_x = x\n","        tanh_out = Conv1D(self.nb_filters, 2, dilation_rate=2 ** i, padding='causal',\n","                          use_bias=self.use_bias,\n","                          name='dilated_conv_%d_tanh_s%d' % (2 ** i, stack_nb), activation='tanh')(x)\n","        sigm_out = Conv1D(self.nb_filters, 2, dilation_rate=2 ** i, padding='causal',\n","                          use_bias=self.use_bias,\n","                          name='dilated_conv_%d_sigm_s%d' % (2 ** i, stack_nb), activation='sigmoid')(x)\n","        x = Multiply(name='gated_activation_%d_s%d' % (i, stack_nb))([tanh_out, sigm_out])\n","\n","        res_x = Conv1D(self.nb_filters, 1, padding='same', use_bias=self.use_bias)(x)\n","        skip_x = Conv1D(self.nb_filters, 1, padding='same', use_bias=self.use_bias)(x)\n","        res_x = Add()([original_x, res_x])\n","        return res_x, skip_x\n","\n","    def build_model(self):\n","        input_layer = Input(shape=self.input_shape, name='input_part')\n","        out = input_layer\n","        skip_connections = []\n","        out = Conv1D(self.nb_filters, 2,\n","                     dilation_rate=1,\n","                     padding='causal',\n","                     name='initial_causal_conv'\n","                     )(out)\n","        for stack_nb in range(self.nb_stacks):\n","            for i in range(0, self.dilation_depth + 1):\n","                out, skip_out = self.residual_block(out, i, stack_nb)\n","                skip_connections.append(skip_out)\n","\n","        if self.use_skip_connections:\n","            out = Add()(skip_connections)\n","        out = Activation('relu')(out)\n","        # added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames\n","        # spanning 10 milliseconds (160× downsampling)\n","        # mean pooling layer adjust pool_size_1 to change downsampling\n","        out = AveragePooling1D(self.pool_size_1, padding='same', name='mean_pooling_layer_downsampling')(out)\n","\n","\n","        # few non-causal convolutions\n","        # few non-causal convolutions\n","        out = Conv1D(self.nb_filters, self.pool_size_1, strides=2, padding='same', activation='relu')(out)\n","        out = Conv1D(self.nb_filters, self.pool_size_2, strides=2, padding='same', activation='relu')(out)\n","        out = Conv1D(self.output_shape, self.pool_size_2, strides=2, padding='same', activation='relu')(out)\n","        out = Conv1D(self.output_shape, self.pool_size_2, strides=2, padding='same', activation='relu')(out)\n","\n","\n","        out = Flatten()(out)\n","        out = Dense(512, activation='relu')(out)\n","        out = Dense(self.output_shape, activation='softmax')(out)\n","\n","        return Model(input_layer, out)\n","\n","    def get_model(self):\n","        return self.model\n","\n","    def get_summary(self):\n","        self.model.summary()\n","\n","    def get_receptive_field(self):\n","        return self.nb_stacks * (2 ** self.dilation_depth * 2) - (self.nb_stacks - 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NGLcMbXnZmjK","colab_type":"text"},"cell_type":"markdown","source":["### Model Parameters"]},{"metadata":{"colab_type":"code","id":"eQZAXGCNAUiQ","colab":{}},"cell_type":"code","source":["# Try all bins to start with\n","bin_range = (0,63)\n","data_shape = (3000, 63)\n","n_filters = 64\n","dilation_depth = 8\n","activation = 'softmax'\n","scale_ratio = 1\n","kernel_size = 2\n","pool_size_1 = 4\n","pool_size_2 = 8\n","batch_size = 16\n","epochs = 20"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PGNLieoovEV-","colab_type":"code","colab":{}},"cell_type":"code","source":["wnc = WaveNetClassifier(data_shape, (7,), kernel_size=kernel_size,\n","                        dilation_depth=dilation_depth, n_filters=n_filters,\n","                        pool_size_1=pool_size_1, pool_size_2=pool_size_2,\n","                        use_bias=False, use_skip_connections=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dfboEAVLZ-rQ","colab_type":"text"},"cell_type":"markdown","source":["### Training on data with MTI filter"]},{"metadata":{"id":"24MmMsO_A2ND","colab_type":"code","colab":{}},"cell_type":"code","source":["wnc.build_model()\n","model = wnc.get_model()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ow03holyAA9F","colab_type":"code","colab":{}},"cell_type":"code","source":["# Parameters\n","params = {'dim': (data_shape),\n","          'batch_size': batch_size,\n","          'n_classes': nb_classes,\n","          'data_directory': DATA_PATH_MTI}\n","# Generators\n","training_generator = DataGenerator(partition['train'], labels, **params, shuffle=True)\n","validation_generator = DataGenerator(partition['validation'], labels, **params, shuffle=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WNw27FjuiZkt","colab_type":"code","colab":{}},"cell_type":"code","source":["load_weights = False\n","weights_path = MODEL_PATH + \"epoch-05-val_acc-0.77.hdf5\"\n","\n","start_epoch = 0\n","if load_weights:\n","    model = load_model(weights_path)\n","    last_epoch = weights_path.split(\"-\")[-3]\n","    start_epoch = int(last_epoch)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XeM1B3UiiafA","colab_type":"code","colab":{}},"cell_type":"code","source":["if not load_weights:\n","    model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v86F1eA7Tn5B","colab_type":"code","colab":{}},"cell_type":"code","source":["checkpoint = ModelCheckpoint(MODEL_PATH + \"epoch-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5\",\n","                             monitor='val_acc', verbose=0, save_best_only=False,\n","                             save_weights_only=False, mode='auto', period=1)\n","\n","csv_logger = CSVLogger(RESULTS_PATH + \"test_4.csv\", append=True)\n","callbacks_list = [checkpoint, csv_logger]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K8efI3NxA4ii","colab_type":"code","outputId":"1956b5a1-6bd9-4855-fe2a-50db7500fa25","executionInfo":{"status":"error","timestamp":1547757842528,"user_tz":0,"elapsed":2754179,"user":{"displayName":"Andrew Mackay","photoUrl":"https://lh3.googleusercontent.com/-24hiGmdxZDE/AAAAAAAAAAI/AAAAAAAAL_I/RW7nqM11LkM/s64/photo.jpg","userId":"06804410358976473893"}},"colab":{"base_uri":"https://localhost:8080/","height":3855}},"cell_type":"code","source":["# Train model on dataset\n","history = model.fit_generator(generator=training_generator,\n","                    validation_data=validation_generator,\n","                    use_multiprocessing=True,\n","                    workers=9,\n","                    epochs=epochs,\n","                    callbacks=callbacks_list,\n","                    initial_epoch=start_epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","2315/2315 [==============================] - 522s 226ms/step - loss: 1.3438 - acc: 0.4094 - val_loss: 1.0707 - val_acc: 0.5570\n","\n","Epoch 2/20\n","2315/2315 [==============================] - 506s 219ms/step - loss: 0.6498 - acc: 0.7089 - val_loss: 0.9194 - val_acc: 0.6530\n","Epoch 3/20\n","2315/2315 [==============================] - 508s 219ms/step - loss: 0.3257 - acc: 0.8702 - val_loss: 1.3848 - val_acc: 0.5692\n","Epoch 4/20\n","2315/2315 [==============================] - 506s 218ms/step - loss: 0.2530 - acc: 0.9168 - val_loss: 0.9152 - val_acc: 0.6880\n","Epoch 5/20\n","2315/2315 [==============================] - 507s 219ms/step - loss: 0.1677 - acc: 0.9381 - val_loss: 0.7156 - val_acc: 0.7728\n","Epoch 6/20\n"," 268/2315 [==>...........................] - ETA: 6:20 - loss: 0.1370 - acc: 0.9499"],"name":"stdout"},{"output_type":"stream","text":["Process ForkPoolWorker-97:\n","Process ForkPoolWorker-107:\n","Process ForkPoolWorker-94:\n","Process ForkPoolWorker-106:\n","Process ForkPoolWorker-108:\n","Process ForkPoolWorker-96:\n","Process ForkPoolWorker-95:\n","Process ForkPoolWorker-93:\n","Process ForkPoolWorker-92:\n","Process ForkPoolWorker-98:\n","Process ForkPoolWorker-91:\n","Process ForkPoolWorker-100:\n","Process ForkPoolWorker-105:\n","Traceback (most recent call last):\n","Process ForkPoolWorker-104:\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","Process ForkPoolWorker-102:\n","Process ForkPoolWorker-101:\n","Process ForkPoolWorker-103:\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","Traceback (most recent call last):\n","Process ForkPoolWorker-99:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","KeyboardInterrupt\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n","    res = self._reader.recv_bytes()\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n","    buf = self._recv_bytes(maxlength)\n","  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","KeyboardInterrupt\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n","    buf = self._recv(4)\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-9226537ec87e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     initial_epoch=start_epoch)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_enqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mval_enqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfinished_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}