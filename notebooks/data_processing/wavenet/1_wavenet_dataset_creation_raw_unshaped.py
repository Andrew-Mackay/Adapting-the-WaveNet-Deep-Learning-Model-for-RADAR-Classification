# -*- coding: utf-8 -*-
"""1_wavenet_dataset_creation_raw_unshaped.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jlA3dvy1UL6aDcy6wFfcnGxRquGC7maK
"""

import os
path = os.getcwd()
if path == '/content':
    from google.colab import drive
    drive.mount('/content/gdrive')
    BASE_PATH = '/content/gdrive/My Drive/Level-4-Project/'
    # !cd gdrive/My\ Drive/Level-4-Project/ && pip install --editable .
    os.chdir('gdrive/My Drive/Level-4-Project/')
    
elif path == 'D:\\Google Drive\\Level-4-Project\\notebooks\\data_processing\\wavenet':
    BASE_PATH = "D:/Google Drive/Level-4-Project/"
    
elif path == "/export/home/2192793m":
    BASE_PATH = "/export/home/2192793m/Level-4-Project/"
    

DATA_PATH = BASE_PATH + 'data/'
RAW_PATH = DATA_PATH + 'raw/raw_converted/'
PROCESSED_PATH = DATA_PATH + 'processed/wavenet/raw_not_reshaped/'
    
from src.data import load_data
from src.visualization import multiple_plots, visualize, plot_confusion_matrix
from src.features import make_spectrograms, process_labels, make_directory

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import mlab
from matplotlib import colors
from scipy.signal import butter, freqz, lfilter, spectrogram
import time
from sklearn import preprocessing

import numpy as np
import sys
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
from keras.utils import np_utils
import sys
from sklearn.metrics import classification_report, confusion_matrix
import csv
from keras.models import load_model
import pickle

labels = ["walking", "pushing", "sitting", "pulling", "circling", "clapping", "bending"]
processed = {"A":{}, "B":{}, "C":{}, "D":{}, "E":{}, "F":{}}
for name, item in processed.items():
    for label in labels:
        item[label] = []

df_labels = pd.read_csv(RAW_PATH + 'Labels.csv')
df_labels.rename(columns={'dataset ID':'dataset_id'}, inplace=True)
df_labels = process_labels.process_labels(df_labels)

WINDOW_LENGTH = 3 # 3 seconds windows
STEP_SIZE = 1000 # 0.1 second steps

number_of_rows = df_labels.shape[0]
current_row = 1
for row in df_labels.itertuples():
    if current_row > 20:
        break
    print(row.user_label, row.aspect_angle, row.label)
    if row.aspect_angle != "0":
        current_row += 1
        continue
    
    print("Processing row", current_row, "of", number_of_rows)
    file_name = RAW_PATH + "Dataset_" + str(row.dataset_id) + ".dat"
    radar_df = pd.read_csv(file_name, header=None)[1]
    
    # Grab RADAR settings from top of file
    center_frequency = float(radar_df.iloc[1])
    sweep_time = float(radar_df.iloc[2])/1000  # convert to seconds
    number_of_time_samples = float(radar_df.iloc[3])
    bandwidth = float(radar_df.iloc[4])
    sampling_frequency = number_of_time_samples/sweep_time
    record_length = 60
    number_of_chirps = record_length/sweep_time
    
    # Put data values into an array
    data = radar_df.iloc[5:].apply(complex).values
    
    # Reshape into chirps over time (means don't need to change step_size calculations just flatten later)
    data_time = np.reshape(data, (int(number_of_chirps),int(number_of_time_samples)))

#     Sanity check to ensure flattening was working correctly
#     plt.plot(abs(data[:384000]))
#     plt.show()

                                         
    window_size = int(WINDOW_LENGTH * 1000)
    iterations = data_time.shape[0] - window_size
    spectrograms = []
    for i in range(0, iterations, STEP_SIZE):
        spectrograms.append(data_time[i:(i + window_size),:].flatten())
        
#     Sanity check part 2        
#     plt.plot(abs(spectrograms[0]))
#     plt.show()

    processed[row.user_label][row.label].append(spectrograms)

    current_row += 1

# Tried pickle but could not handle the large size. Now try using Numpy save
# np.save(PROCESSED_PATH + 'raw_not_reshaped.npy', processed)
# Due to memory errors, must be done in stages
# np.save(PROCESSED_PATH + '1-20.npy', processed) # 1 - 20
# np.save(PROCESSED_PATH + 'raw_not_reshaped_part2.npy', processed) # 41 - 80
# np.save(PROCESSED_PATH + 'raw_not_reshaped_part3.npy', processed) # 81 - 123



with open(PROCESSED_PATH + "1-20.pkl", "wb") as file:
    pickle.dump(processed, file)