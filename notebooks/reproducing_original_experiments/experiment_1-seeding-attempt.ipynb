{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1553,
     "status": "ok",
     "timestamp": 1540329203407,
     "user": {
      "displayName": "Andrew Mackay",
      "photoUrl": "",
      "userId": "06804410358976473893"
     },
     "user_tz": -60
    },
    "id": "0_-KJbyNnPqf",
    "outputId": "9a05e376-fcb3-4ca5-c1ef-634e577915e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnU4VGgjH1mW"
   },
   "source": [
    "# Experiment 1: Six different depths of models, trained on 5 of the 6 subjects then evaluated on the 6th. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sqmYc1yQb-k"
   },
   "source": [
    "*   0 degree aspect angle\n",
    "*   Models:\n",
    "    *  2-4\n",
    "    *  4-8\n",
    "    *  8-16\n",
    "    *  16-32\n",
    "    *  32-64\n",
    "    *  64-128\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZxhO7V0ZHUE"
   },
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5jAnxaaZLzZ"
   },
   "source": [
    "Allow editing of modules using editor (auto reloading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGNeUj-JDXhs"
   },
   "outputs": [],
   "source": [
    "# Needed to allow editing using PyCharm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwLbqieVYIJt"
   },
   "source": [
    "Needed for compatibility when using both CoLab and Local Jupyter notebook. It sets the appropriate file path for the data and also installs local packages such as models and data_loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5312,
     "status": "ok",
     "timestamp": 1540329209854,
     "user": {
      "displayName": "Andrew Mackay",
      "photoUrl": "",
      "userId": "06804410358976473893"
     },
     "user_tz": -60
    },
    "id": "3XeU0HtoDXh6",
    "outputId": "15417c20-1d33-4bb2-ce97-9498d77e60d8"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "if os.getcwd() == '/content':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    FILE_PATH = '/content/gdrive/My Drive/Level-4-Project/data/'\n",
    "    !cd gdrive/My\\ Drive/Level-4-Project/ && pip install --editable .\n",
    "    os.chdir('gdrive/My Drive/Level-4-Project/')\n",
    "    \n",
    "else:\n",
    "    FILE_PATH = \"C:/Users/macka/Google Drive/Level-4-Project/data/\"\n",
    "    \n",
    "# from src.models.original_models import cnn_various_depths\n",
    "# from src.data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgYwaq1eZb5u"
   },
   "source": [
    "Import remaining packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8rzmlEhpe_R"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import sys\n",
    "from six.moves import cPickle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import sys\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import csv\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsHOJ9lEpe_V"
   },
   "outputs": [],
   "source": [
    "# Needed as originally code was for theano backend but now using tensor flow\n",
    "# from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NADnGONvrZzD"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "\n",
    "def make_model(depth, img_rows, img_cols, nb_classes):\n",
    "    model = Sequential(name=depth)\n",
    "    if depth == \"2-4\":\n",
    "        model.add(Convolution2D(2, (3, 3), padding='same', input_shape=(1, img_rows, img_cols), activation='relu'))\n",
    "        model.add(Convolution2D(2, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Convolution2D(4, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Convolution2D(4, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    elif depth == \"4-8\":\n",
    "        model.add(Convolution2D(4, (3, 3), padding='same', input_shape=(1, img_rows, img_cols), activation='relu'))\n",
    "        model.add(Convolution2D(4, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Convolution2D(8, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Convolution2D(8, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    elif depth == \"8-16\":\n",
    "        model.add(Convolution2D(8, (3, 3), padding='same', input_shape=(1, img_rows, img_cols), activation='relu'))\n",
    "        model.add(Convolution2D(8, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Convolution2D(16, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Convolution2D(16, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    elif depth == \"16-32\":\n",
    "        model.add(Convolution2D(16, (3, 3), padding='same', input_shape=(1, img_rows, img_cols), activation='relu'))\n",
    "        model.add(Convolution2D(16, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Convolution2D(32, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    elif depth == \"32-64\":\n",
    "        model.add(Convolution2D(32, (3, 3), padding='same', input_shape=(1, img_rows, img_cols), activation='relu'))\n",
    "        model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Convolution2D(64, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    elif depth == \"64-128\":\n",
    "        model.add(Convolution2D(64, (3, 3), padding='same', input_shape=(1, img_rows, img_cols), activation='relu'))\n",
    "        model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Convolution2D(128, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azxJ2f6urcMe"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from six.moves import cPickle\n",
    "\n",
    "def load_batch(fpath, label_key='labels'):\n",
    "    f = open(fpath, 'rb')\n",
    "    if sys.version_info < (3,):\n",
    "        d = cPickle.load(f)\n",
    "    else:\n",
    "        d = cPickle.load(f, encoding=\"bytes\")\n",
    "        # decode utf8\n",
    "        for k, v in d.items():\n",
    "            # added check as otherwise tries to decode a string\n",
    "            if type(k) is not str:\n",
    "                del(d[k])\n",
    "                d[k.decode(\"utf8\")] = v\n",
    "    f.close()\n",
    "    data = d[\"data\"]\n",
    "    labels = d[label_key]\n",
    "\n",
    "    data = data.reshape(data.shape[0], 1, 75, 75)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_data(path, nb_train_samples):\n",
    "    X_train = np.zeros((nb_train_samples, 1, 75, 75), dtype=\"uint8\")\n",
    "    y_train = np.zeros((nb_train_samples,), dtype=\"uint8\")\n",
    "    offset = int(nb_train_samples/5)\n",
    "    for i in range(1, 6):\n",
    "        fpath = os.path.join(path, 'data_batch_' + str(i))\n",
    "        data, labels = load_batch(fpath)\n",
    "        X_train[(i-1)*offset:i*offset, :, :, :] = data\n",
    "        y_train[(i-1)*offset:i*offset] = labels\n",
    "\n",
    "    fpath = os.path.join(path, 'test_batch')\n",
    "    X_test, y_test = load_batch(fpath)\n",
    "\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oqq9LWY5ZiIB"
   },
   "source": [
    "## Experiment Setup and Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5r9302zCpe_j"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "nb_classes = 7\n",
    "nb_epoch = 20\n",
    "nb_epoch = 1\n",
    "data_augmentation = False\n",
    "nb_train_samples = 35595\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 75, 75\n",
    "# the CIFAR10 images are RGB\n",
    "\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = load_data((FILE_PATH + 'cifar_initialised'), nb_train_samples)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255 \n",
    "\n",
    "models = {\"2-4\": {}, \"4-8\": {}, \"8-16\": {}, \"16-32\": {}, \"32-64\": {}, \"64-128\": {}}\n",
    "for name, model in models.items():\n",
    "    model[\"model\"] = make_model(name, img_rows, img_cols, nb_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjJ4B23bZz47"
   },
   "source": [
    "## Training and Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1745
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 646761,
     "status": "error",
     "timestamp": 1540329868673,
     "user": {
      "displayName": "Andrew Mackay",
      "photoUrl": "",
      "userId": "06804410358976473893"
     },
     "user_tz": -60
    },
    "id": "pKdiTJP6pe_n",
    "outputId": "aeb5f9b5-411f-41ec-b1ba-00e9c327c0ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 35595 samples, validate on 7119 samples\n",
      "Epoch 1/1\n",
      "34900/35595 [============================>.] - ETA: 0s - loss: 0.6652 - acc: 0.7398"
     ]
    }
   ],
   "source": [
    "for name, model_data in models.items():\n",
    "    # let's train the model using SGD + momentum (how original).\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model_data[\"model\"].compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        history = model_data[\"model\"].fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=nb_epoch,\n",
    "            validation_data=(X_test, Y_test),\n",
    "            shuffle=True, \n",
    "            verbose=1)\n",
    "\n",
    "        y_pred = model_data[\"model\"].predict_classes(X_test)\n",
    "        target_names = ['ArmFasterTowards', 'ArmSlowerTowards', 'CirclingArm', 'Clapping', 'PickingUp', 'Sitting', 'Walking']\n",
    "        model_data[\"history\"] = history\n",
    "        model_data[\"classification_report\"] = classification_report(np.argmax(Y_test,axis=1), y_pred,target_names=target_names)\n",
    "        model_data[\"confusion_matrix\"] = confusion_matrix(np.argmax(Y_test,axis=1), y_pred)\n",
    "#         model.save('/home/aleksandar/sets/cifar_initialised/results/model_initialised.h5')\n",
    "\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "\n",
    "        # this will do preprocessing and realtime data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "\n",
    "        # compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied)\n",
    "        datagen.fit(X_train)\n",
    "\n",
    "        # fit the model on the batches generated by datagen.flow()\n",
    "        hist = model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                            batch_size=batch_size),\n",
    "                            samples_per_epoch=X_train.shape[0],\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            validation_data=(X_test, Y_test))\n",
    "\n",
    "        #NEW--------------------\n",
    "        y_pred = model.predict_classes(X_test)\n",
    "        print(y_pred)\n",
    "        target_names = ['ArmFasterTowards', 'ArmSlowerTowards', 'CirclingArm', 'Clapping', 'PickingUp', 'Sitting', 'Walking']\n",
    "#         sys.stdout = open('/home/aleksandar/sets/cifar_initialised/results/report_initialised.txt', \"w\")\n",
    "        print(\"CLASSIFICATION REPORT:\")\n",
    "        print(classification_report(np.argmax(Y_test,axis=1), y_pred,target_names=target_names))\n",
    "        print (\"\\n\")\n",
    "        print(\"CONFUSION MATRIX:\")\n",
    "        print(confusion_matrix(np.argmax(Y_test,axis=1), y_pred))\n",
    "\n",
    "#         with open('/home/aleksandar/sets/cifar_initialised/results/graphs_initialised.csv', \"w\") as f:\n",
    "#            w = csv.writer(f)\n",
    "#            train_loss = hist.history['loss']\n",
    "#            val_loss = hist.history['val_loss']\n",
    "#            train_acc = hist.history['acc']\n",
    "#            val_acc = hist.history['val_acc']\n",
    "#            helplist = list(zip(train_loss, train_acc, val_loss, val_acc))\n",
    "#            w.writerow(\"train_loss, train_acc, val_loss, val_acc\")\n",
    "#            for val in helplist:\n",
    "#               w.writerow([val])\n",
    "\n",
    "\n",
    "#         model.save('/home/aleksandar/sets/cifar_initialised/results/model_initialised.h5')\n",
    "#         #-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uU-i7-ijaNu-"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1540325165056,
     "user": {
      "displayName": "Andrew Mackay",
      "photoUrl": "",
      "userId": "06804410358976473893"
     },
     "user_tz": -60
    },
    "id": "_pj36NuhaRMU",
    "outputId": "791d0b7d-b5ea-4a7a-9b70-dde37d64ac7b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name, model_data in models.items():\n",
    "    plt.plot(model_data[\"history\"].history['val_acc'], label=name)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.title('Experiment 1 (Figure 3a)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PO4Wg4h3pJz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "experiment_1-seeding-attempt.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
